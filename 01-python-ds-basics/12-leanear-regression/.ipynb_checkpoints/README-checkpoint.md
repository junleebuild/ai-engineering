# ğŸ§  Linear Regression (Part 02)

## ğŸ“Œ Overview

In this section, I studied **Linear Regression**, one of the most fundamental supervised machine learning algorithms.  
Itâ€™s used to predict a continuous outcome based on the relationship between input variables (features) and the target variable.

I used the **scikit-learn** library (`sklearn`) to perform the steps of training, evaluating, and visualizing linear regression.

---

## ğŸ“ Files Included

- `01-my linear regression with python.ipynb` â€“ My personal practice notebook  
- `02-linear regression project.ipynb` â€“ Project assignment file  
- `03-linear regression project - solutions.ipynb` â€“ Instructorâ€™s reference solution  
- `01-ref linear regression with python.ipynb` â€“ Instructor's lecture notebook  
- `USA_Housing.csv` â€“ Dataset for the initial regression model  
- `Ecommerce Customers` â€“ Dataset for the project

---

## ğŸ”§ Key Concepts and Workflow

### 1. ğŸ“Š Train-Test Split

```python
from sklearn.model_selection import train_test_split

X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
        'Avg. Area Number of Bedrooms', 'Area Population']]
y = df['Price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
```

We split the dataset into:
- `X_train`: Training features  
- `y_train`: Training labels  
- `X_test`: Test features  
- `y_test`: Test labels

> ğŸ” **Goal**: Teach the model with training data and evaluate its performance on test data.

---

### 2. ğŸ§  Model Training

```python
from sklearn.linear_model import LinearRegression

lm = LinearRegression()
lm.fit(X_train, y_train)
```

- The `.fit()` function **trains the model** by finding the best-fitting line through the training data.
- Internally, it calculates the **optimal coefficients (slope)** and **intercept** for the regression equation:

> ğŸ“˜ ğ‘¦ = ğ›½â‚€ + ğ›½â‚ğ‘‹â‚ + ğ›½â‚‚ğ‘‹â‚‚ + â‹¯ + ğ›½â‚™ğ‘‹â‚™

---

### 3. ğŸ§® Interpreting Coefficients

```python
coeff_df = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])
```

Each coefficient tells us **how much the target variable (`Price`) will change** when the corresponding feature increases by one unit, **keeping all others constant**.

---

### 4. ğŸ“ˆ Model Evaluation

We predicted outcomes for test data and evaluated the model:

```python
predictions = lm.predict(X_test)

from sklearn import metrics

print("MAE:", metrics.mean_absolute_error(y_test, predictions))
print("MSE:", metrics.mean_squared_error(y_test, predictions))
print("RMSE:", np.sqrt(metrics.mean_squared_error(y_test, predictions)))
```

**Evaluation Metrics:**
- MAE: Mean Absolute Error
- MSE: Mean Squared Error
- RMSE: Root Mean Squared Error

---

### 5. ğŸ“Š Visualization (Optional)

```python
sns.scatterplot(x=y_test, y=predictions)
```

Shows how closely the predicted values match the actual target values.

---

## ğŸ’¬ Personal Reflection

I finally started understanding what machine learning actually **does** behind the scenes.  
The `.fit()` function felt like the model was â€œstudyingâ€ the patterns in data, and then `.predict()` let it guess the outcomes based on that study.

Even though I didnâ€™t fully understand every mathematical detail, the overall process of **train â†’ fit â†’ predict â†’ evaluate** makes intuitive sense now.  
Looking at `coef_` also helped me see how features influence the target, which I think is very powerful in real-life applications like housing or sales predictions.
