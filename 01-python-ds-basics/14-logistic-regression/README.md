
# ü§ñ 13 - Logistic Regression 

## üìå Overview

In this section, I studied **Logistic Regression**, one of the most widely used supervised classification algorithms.  
Unlike linear regression (which predicts a continuous number), logistic regression predicts a **probability**, allowing it to classify between two categories (binary classification).

Common examples:
- Will a user click this ad or not?
- Is this email spam or not?
- Will this customer churn or not?

---

## ‚öôÔ∏è Key Concepts

- The output of logistic regression is **between 0 and 1** ‚Äî a probability.
- The model uses a **sigmoid (logistic) function** to map predictions to probabilities:

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

- If the probability is greater than 0.5 ‚Üí predict **class 1**  
  Else ‚Üí predict **class 0**

---

## üßë‚Äçüíª Project: Predicting Ad Clicks

### Dataset: `advertising.csv`

### Features:
- **Daily Time Spent on Site**  
- **Age**  
- **Area Income**  
- **Daily Internet Usage**  
- **Ad Topic Line**  
- **City**  
- **Male**  
- **Timestamp**  
- **Clicked on Ad** (Target variable - 0 or 1)

---

### üìä Workflow:

1Ô∏è‚É£ **EDA (Exploratory Data Analysis)**  
- Used `sns.jointplot()`, `sns.pairplot()` to understand feature relationships.

2Ô∏è‚É£ **Data Preparation**  
- Selected features for the model.
- Splitted the data into training/test sets:
    ```python
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
    ```

3Ô∏è‚É£ **Training the Model**
    ```python
    from sklearn.linear_model import LogisticRegression

    logmodel = LogisticRegression(max_iter=1000)
    logmodel.fit(X_train, y_train)
    ```

4Ô∏è‚É£ **Prediction**
    ```python
    predictions = logmodel.predict(X_test)
    ```

5Ô∏è‚É£ **Evaluation**
    ```python
    from sklearn.metrics import classification_report, confusion_matrix

    print(classification_report(y_test, predictions))
    print(confusion_matrix(y_test, predictions))
    ```

---

## üí¨ Personal Reflection

Logistic Regression helped me understand **classification vs regression** more clearly.  
Seeing how logistic regression predicts probabilities (not raw scores), and then uses the sigmoid function to classify was very useful.  
While I still need more practice with interpreting the confusion matrix and precision/recall, the process of training and evaluating a classifier is becoming more comfortable.

---

## üìÅ Files Included

- `01-logistic regression.ipynb` ‚Äì Learning notebook  
- `02-logistic regression project.ipynb` ‚Äì Project notebook  
- `advertising.csv` ‚Äì Dataset for ad click prediction
- `titanic_test.csv`
- `titanic_train.csv`
---
